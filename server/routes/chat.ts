import { RequestHandler } from "express";
import { withOpenAIClient } from "../lib/openai-key-manager";
import { logAIUsage } from "../lib/ai-usage";

interface ChatRequest {
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
}

export const handleChatCompletion: RequestHandler = async (req, res) => {
  try {
    const { messages } = req.body as ChatRequest;

    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({
        error: 'Invalid request: messages array is required'
      });
    }

    // Call OpenAI API via key manager (Appwrite collection + env fallback)
    const completion = await withOpenAIClient((client) => client.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: messages,
      temperature: 0.7,
      max_tokens: 500,
    }));

    const response = {
      message: completion.choices[0]?.message?.content || 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù…Ø§ Ù‚Ø¯Ø±ØªØ´ Ø£ÙÙ‡Ù…. Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ ğŸ™',
      usage: completion.usage,
    };

    // Log AI usage (best-effort)
    try {
      const usage = completion.usage as any;
      await logAIUsage({
        feature: 'chat',
        route: '/api/chat',
        model: (completion as any).model || 'gpt-4o-mini',
        tokensPrompt: usage?.prompt_tokens,
        tokensCompletion: usage?.completion_tokens,
        tokensTotal: usage?.total_tokens,
        userId: null,
        metadata: null,
      });
    } catch (e) {
      // Ù„Ø§ Ù†Ù…Ù†Ø¹ Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
      console.error('Failed to log AI usage for /api/chat:', e);
    }

    res.json(response);
  } catch (error: any) {
    console.error('OpenAI API Error:', error);
    
    res.status(500).json({
      error: 'Ø­ØµÙ„ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø¯Ù…Ø©. Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ Ø¨Ø¹Ø¯ Ø´ÙˆÙŠØ© ğŸ™',
      details: error.message
    });
  }
};
